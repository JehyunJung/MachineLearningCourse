{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 순환신경망 구현 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "NUM_WORDS = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb=tf.keras.layers.Embedding(NUM_WORDS,16) #embedding?  길이가 32인 숫자 나열 형태를 그대로 학습하는 것은 좋지 않다 \n",
    "        self.lstm=tf.keras.layers.LSTM(32)\n",
    "        self.dense=tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "        \n",
    "    def call(self,x,training=None,mask=None):\n",
    "        x=self.emb(x)\n",
    "        x=self.lstm(x)\n",
    "        return self.dense(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 준비\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218 189\n"
     ]
    }
   ],
   "source": [
    "imdb=tf.keras.datasets.imdb\n",
    "(x_train,y_train),(x_test,y_test)=imdb.load_data(num_words=NUM_WORDS) #x_train은 문장으로 이루어져 있고, y_train은 긍정/부정 즉 binary형태의 data, num_words을 통해 문장에 사용되는 단어의 개수 제한\n",
    "print(len(x_train[0]),len(x_train[1]))\n",
    "# x_train은 길이가 다양한 문장이다. -> 그래서를 이를 길이가 32인 문장들로 만들어 주었다.\n",
    "x_train=tf.keras.preprocessing.sequence.pad_sequences(x_train,value=0,padding='pre',maxlen=32) #길이가 32인 문장이 되도록 전처리 작업 진행, 만약 길이가 32보다 작은 경우에 대해서는 앞에 0으로 padding\n",
    "x_test=tf.keras.preprocessing.sequence.pad_sequences(x_test,value=0,padding='pre',maxlen=32)\n",
    "\n",
    "train_ds=tf.data.Dataset.from_tensor_slices((x_train,y_train)).shuffle(1000).batch(32)\n",
    "test_ds=tf.data.Dataset.from_tensor_slices((x_test,y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  30 5535   18   51   36   28  224   92   25  104    4  226   65   16\n",
      "   38 1334   88   12   16  283    5   16 4472  113  103   32   15   16\n",
      " 5345   19  178   32]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 루프 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 782 steps, validate for 782 steps\n",
      "Epoch 1/10\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.4232 - accuracy: 0.8036 - val_loss: 0.4417 - val_accuracy: 0.7911\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.3548 - accuracy: 0.8447 - val_loss: 0.4476 - val_accuracy: 0.7863\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.3140 - accuracy: 0.8672 - val_loss: 0.4737 - val_accuracy: 0.7799\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.2733 - accuracy: 0.8892 - val_loss: 0.5163 - val_accuracy: 0.7760\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.2262 - accuracy: 0.9116 - val_loss: 0.5968 - val_accuracy: 0.7677\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.1803 - accuracy: 0.9331 - val_loss: 0.6577 - val_accuracy: 0.7644\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.1432 - accuracy: 0.9476 - val_loss: 0.7996 - val_accuracy: 0.7556\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 18s 23ms/step - loss: 0.1111 - accuracy: 0.9619 - val_loss: 0.8417 - val_accuracy: 0.7505\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 20s 26ms/step - loss: 0.0927 - accuracy: 0.9668 - val_loss: 0.9690 - val_accuracy: 0.7480\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.0728 - accuracy: 0.9742 - val_loss: 1.1573 - val_accuracy: 0.7460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2448ee91a08>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, validation_data=test_ds, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU(Gated Recurrent Unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb=tf.keras.layers.Embedding(NUM_WORDS,16) #embedding?  길이가 32인 숫자 나열 형태를 그대로 학습하는 것은 좋지 않다 각각의 NUM_WORDS에 대해서 16 길이의 벡터로 만들어진다.? 독립성 유지??\n",
    "        self.gru=tf.keras.layers.GRU(32)\n",
    "        self.dense=tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "        \n",
    "    def call(self,x,training=None,mask=None):\n",
    "        x=self.emb(x)\n",
    "        x=self.gru(x)\n",
    "        return self.dense(x)\n",
    "model = MyModel()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_ds, validation_data=test_ds, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb=tf.keras.layers.Embedding(NUM_WORDS,16) #embedding?  길이가 32인 숫자 나열 형태를 그대로 학습하는 것은 좋지 않다 \n",
    "        self.rnn=tf.keras.layers.SimpleRNN(32)\n",
    "        self.dense=tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "        \n",
    "    def call(self,x,training=None,mask=None):\n",
    "        x=self.emb(x)\n",
    "        x=self.rnn(x)\n",
    "        return self.dense(x)\n",
    "model = MyModel()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_ds, validation_data=test_ds, epochs=EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
